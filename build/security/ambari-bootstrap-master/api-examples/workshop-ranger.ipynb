{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "You are authenticated to Ambari!\n"
     ]
    }
   ],
   "source": [
    "### Authenticate to Ambari\n",
    "\n",
    "#### Python requirements\n",
    "import difflib\n",
    "import getpass\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#### Change these to fit your Ambari configuration\n",
    "ambari_protocol = 'http'\n",
    "ambari_server = 'p-lab06.cloudapp.net'\n",
    "ambari_port = 8080\n",
    "ambari_user = 'admin'\n",
    "#cluster = 'Sandbox'\n",
    "\n",
    "#### Above input gives us http://user:pass@hostname:port/api/v1/\n",
    "api_url = ambari_protocol + '://' + ambari_server + ':' + str(ambari_port)\n",
    "\n",
    "#### Prompt for password & build the HTTP session\n",
    "ambari_pass = getpass.getpass()\n",
    "s = requests.Session()\n",
    "s.auth = (ambari_user, ambari_pass)\n",
    "s.headers.update({'X-Requested-By':'seanorama'})\n",
    "\n",
    "#### Authenticate & verify authentication\n",
    "r = s.get(api_url + '/api/v1/clusters')\n",
    "assert r.status_code == 200\n",
    "print(\"You are authenticated to Ambari!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p-lab06'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Set cluster based on existing cluster\n",
    "    \n",
    "cluster = r.json()['items'][0]['Clusters']['cluster_name']\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure ranger-hdfs-audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get current configuration tag\n",
    "config = 'ranger-hdfs-audit'\n",
    "r = s.get(api_url + '/api/v1/clusters/' + cluster + '?fields=Clusters/desired_configs/' + config)\n",
    "assert r.status_code == 200\n",
    "tag = r.json()['Clusters']['desired_configs'][config]['tag']\n",
    "\n",
    "## Get current configuration\n",
    "r = s.get(api_url + '/api/v1/clusters/' + cluster + '/configurations?type=' + config '&tag=' + tag)\n",
    "assert r.status_code == 200\n",
    "#print(json.dumps(r.json(), indent=2))\n",
    "\n",
    "## Update config\n",
    "config_old = r.json()['items'][0]\n",
    "config_new = r.json()['items'][0]\n",
    "\n",
    "#### Make your changes here\n",
    "config_new['properties']['xasecure.audit.destination.db'] = \"true\"\n",
    "print(json.dumps(config_new, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Show the differences\n",
    "a = json.dumps(config_old, indent=2).splitlines(1)\n",
    "b = json.dumps(config_new, indent=2).splitlines(1)\n",
    "\n",
    "for line in difflib.unified_diff(a, b):\n",
    "     sys.stdout.write(line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Clusters\": {\n",
      "    \"desired_config\": {\n",
      "      \"properties\": {\n",
      "        \"xasecure.audit.destination.db\": \"true\",\n",
      "        \"xasecure.audit.destination.db.jdbc.driver\": \"{{jdbc_driver}}\",\n",
      "        \"xasecure.audit.destination.hdfs\": \"true\",\n",
      "        \"xasecure.audit.credential.provider.file\": \"jceks://file{{credential_file}}\",\n",
      "        \"xasecure.audit.destination.hdfs.dir\": \"hdfs://NAMENODE_HOSTNAME:8020/ranger/audit\",\n",
      "        \"xasecure.audit.destination.db.batch.filespool.dir\": \"/var/log/hadoop/hdfs/audit/db/spool\",\n",
      "        \"xasecure.audit.destination.solr.zookeepers\": \"none\",\n",
      "        \"xasecure.audit.destination.db.user\": \"{{xa_audit_db_user}}\",\n",
      "        \"xasecure.audit.destination.solr\": \"false\",\n",
      "        \"xasecure.audit.destination.solr.urls\": \"{{ranger_audit_solr_urls}}\",\n",
      "        \"xasecure.audit.destination.solr.batch.filespool.dir\": \"/var/log/hadoop/hdfs/audit/solr/spool\",\n",
      "        \"xasecure.audit.destination.db.jdbc.url\": \"{{audit_jdbc_url}}\",\n",
      "        \"xasecure.audit.is.enabled\": \"true\",\n",
      "        \"xasecure.audit.provider.summary.enabled\": \"false\",\n",
      "        \"xasecure.audit.destination.hdfs.batch.filespool.dir\": \"/var/log/hadoop/hdfs/audit/hdfs/spool\",\n",
      "        \"xasecure.audit.destination.db.password\": \"crypted\"\n",
      "      },\n",
      "      \"type\": \"ranger-hdfs-audit\",\n",
      "      \"tag\": \"version1436639736138720000\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#### Manipulate the document to match the format Ambari expects\n",
    "\n",
    "#### Adds new configuration tag, deletes fields, and wraps in appropriate json\n",
    "config_new['tag'] = 'version' + str(int(round(time.time() * 1000000000)))\n",
    "del config_new['Config']\n",
    "del config_new['href']\n",
    "del config_new['version']\n",
    "config_new = {\"Clusters\": {\"desired_config\": config_new}}\n",
    "\n",
    "print(json.dumps(config_new, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06\n",
      "200\n",
      "Configuration changed successfully!\n",
      "{\n",
      "  \"resources\": [\n",
      "    {\n",
      "      \"configurations\": [\n",
      "        {\n",
      "          \"stackId\": {\n",
      "            \"stackId\": \"HDP-2.3\",\n",
      "            \"stackName\": \"HDP\",\n",
      "            \"stackVersion\": \"2.3\"\n",
      "          },\n",
      "          \"type\": \"ranger-hdfs-audit\",\n",
      "          \"versionTag\": \"version1436639736138720000\",\n",
      "          \"configs\": {\n",
      "            \"xasecure.audit.destination.db\": \"true\",\n",
      "            \"xasecure.audit.destination.db.jdbc.driver\": \"{{jdbc_driver}}\",\n",
      "            \"xasecure.audit.destination.hdfs\": \"true\",\n",
      "            \"xasecure.audit.is.enabled\": \"true\",\n",
      "            \"xasecure.audit.destination.hdfs.dir\": \"hdfs://NAMENODE_HOSTNAME:8020/ranger/audit\",\n",
      "            \"xasecure.audit.destination.db.batch.filespool.dir\": \"/var/log/hadoop/hdfs/audit/db/spool\",\n",
      "            \"xasecure.audit.provider.summary.enabled\": \"false\",\n",
      "            \"xasecure.audit.destination.db.user\": \"{{xa_audit_db_user}}\",\n",
      "            \"xasecure.audit.destination.solr\": \"false\",\n",
      "            \"xasecure.audit.destination.solr.urls\": \"{{ranger_audit_solr_urls}}\",\n",
      "            \"xasecure.audit.destination.solr.batch.filespool.dir\": \"/var/log/hadoop/hdfs/audit/solr/spool\",\n",
      "            \"xasecure.audit.destination.db.jdbc.url\": \"{{audit_jdbc_url}}\",\n",
      "            \"xasecure.audit.credential.provider.file\": \"jceks://file{{credential_file}}\",\n",
      "            \"xasecure.audit.destination.solr.zookeepers\": \"none\",\n",
      "            \"xasecure.audit.destination.hdfs.batch.filespool.dir\": \"/var/log/hadoop/hdfs/audit/hdfs/spool\",\n",
      "            \"xasecure.audit.destination.db.password\": \"crypted\"\n",
      "          },\n",
      "          \"serviceConfigVersions\": null,\n",
      "          \"version\": 3,\n",
      "          \"clusterName\": \"p-lab06\",\n",
      "          \"configAttributes\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"service_config_version_note\": null,\n",
      "      \"group_id\": -1,\n",
      "      \"href\": \"http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06/configurations/service_config_versions?service_name=HDFS&service_config_version=7\",\n",
      "      \"service_name\": \"HDFS\",\n",
      "      \"service_config_version\": 7,\n",
      "      \"group_name\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "body = config_new\n",
    "r = s.put(api_url + '/api/v1/clusters/' + cluster, data=json.dumps(body))\n",
    "\n",
    "print(r.url)\n",
    "print(r.status_code)\n",
    "assert r.status_code == 200\n",
    "print(\"Configuration changed successfully!\")\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure ranger-hdfs-plugin-properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-65449610bfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Get current configuration tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ranger-hdfs-plugin-properties'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/api/v1/clusters/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'?fields=Clusters/desired_configs/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clusters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'desired_configs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "## Get current configuration tag\n",
    "config = 'ranger-hdfs-plugin-properties'\n",
    "r = s.get(api_url + '/api/v1/clusters/' + cluster + '?fields=Clusters/desired_configs/' + config)\n",
    "assert r.status_code == 200\n",
    "tag = r.json()['Clusters']['desired_configs'][config]['tag']\n",
    "\n",
    "## Get current configuration\n",
    "r = s.get(api_url + '/api/v1/clusters/' + cluster + '/configurations?type=' + config + '&tag=' + tag)\n",
    "assert r.status_code == 200\n",
    "print(json.dumps(r.json(), indent=2))\n",
    "\n",
    "## Update config\n",
    "config_old = r.json()['items'][0]\n",
    "config_new = r.json()['items'][0]\n",
    "\n",
    "#### Make your changes here\n",
    "config_new['properties']['ranger-hdfs-plugin-enabled'] = \"Yes\"\n",
    "config_new['properties']['common.name.for.certificate'] = \" \"\n",
    "config_new['properties']['REPOSITORY_CONFIG_USERNAME'] = \"rangeradmin\"\n",
    "rangeradmin_pass = getpass.getpass()\n",
    "config_new['properties']['REPOSITORY_CONFIG_PASSWORD'] = rangeradmin_pass\n",
    "\n",
    "print(json.dumps(config_new, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "+++ \n",
      "@@ -1,11 +1,11 @@\n",
      " {\n",
      "   \"properties\": {\n",
      "-    \"REPOSITORY_CONFIG_PASSWORD\": \"hadoop\",\n",
      "+    \"REPOSITORY_CONFIG_PASSWORD\": \"Hortonworks1!\",\n",
      "     \"policy_user\": \"ambari-qa\",\n",
      "     \"hadoop.rpc.protection\": \"\",\n",
      "-    \"common.name.for.certificate\": \"\",\n",
      "-    \"ranger-hdfs-plugin-enabled\": \"No\",\n",
      "-    \"REPOSITORY_CONFIG_USERNAME\": \"hadoop\"\n",
      "+    \"common.name.for.certificate\": \" \",\n",
      "+    \"ranger-hdfs-plugin-enabled\": \"Yes\",\n",
      "+    \"REPOSITORY_CONFIG_USERNAME\": \"rangeradmin\"\n",
      "   },\n",
      "   \"type\": \"ranger-hdfs-plugin-properties\",\n",
      "   \"href\": \"http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06/configurations?type=ranger-hdfs-plugin-properties&tag=TOPOLOGY_RESOLVED\",\n"
     ]
    }
   ],
   "source": [
    "#### Show the differences\n",
    "a = json.dumps(config_old, indent=2).splitlines(1)\n",
    "b = json.dumps(config_new, indent=2).splitlines(1)\n",
    "\n",
    "for line in difflib.unified_diff(a, b):\n",
    "     sys.stdout.write(line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Clusters\": {\n",
      "    \"desired_config\": {\n",
      "      \"properties\": {\n",
      "        \"REPOSITORY_CONFIG_PASSWORD\": \"Hortonworks1!\",\n",
      "        \"policy_user\": \"ambari-qa\",\n",
      "        \"hadoop.rpc.protection\": \"\",\n",
      "        \"common.name.for.certificate\": \" \",\n",
      "        \"ranger-hdfs-plugin-enabled\": \"Yes\",\n",
      "        \"REPOSITORY_CONFIG_USERNAME\": \"rangeradmin\"\n",
      "      },\n",
      "      \"type\": \"ranger-hdfs-plugin-properties\",\n",
      "      \"tag\": \"version1436640228430000128\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#### Manipulate the document to match the format Ambari expects\n",
    "\n",
    "#### Adds new configuration tag, deletes fields, and wraps in appropriate json\n",
    "config_new['tag'] = 'version' + str(int(round(time.time() * 1000000000)))\n",
    "del config_new['Config']\n",
    "del config_new['href']\n",
    "del config_new['version']\n",
    "config_new = {\"Clusters\": {\"desired_config\": config_new}}\n",
    "\n",
    "print(json.dumps(config_new, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06\n",
      "200\n",
      "Configuration changed successfully!\n",
      "{\n",
      "  \"resources\": [\n",
      "    {\n",
      "      \"configurations\": [\n",
      "        {\n",
      "          \"stackId\": {\n",
      "            \"stackId\": \"HDP-2.3\",\n",
      "            \"stackName\": \"HDP\",\n",
      "            \"stackVersion\": \"2.3\"\n",
      "          },\n",
      "          \"type\": \"ranger-hdfs-plugin-properties\",\n",
      "          \"versionTag\": \"version1436640228430000128\",\n",
      "          \"configs\": {\n",
      "            \"ranger-hdfs-plugin-enabled\": \"Yes\",\n",
      "            \"policy_user\": \"ambari-qa\",\n",
      "            \"common.name.for.certificate\": \" \",\n",
      "            \"hadoop.rpc.protection\": \"\",\n",
      "            \"REPOSITORY_CONFIG_USERNAME\": \"rangeradmin\",\n",
      "            \"REPOSITORY_CONFIG_PASSWORD\": \"Hortonworks1!\"\n",
      "          },\n",
      "          \"serviceConfigVersions\": null,\n",
      "          \"version\": 3,\n",
      "          \"clusterName\": \"p-lab06\",\n",
      "          \"configAttributes\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"service_config_version_note\": null,\n",
      "      \"group_id\": -1,\n",
      "      \"href\": \"http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06/configurations/service_config_versions?service_name=HDFS&service_config_version=8\",\n",
      "      \"service_name\": \"HDFS\",\n",
      "      \"service_config_version\": 8,\n",
      "      \"group_name\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "body = config_new\n",
    "r = s.put(api_url + '/api/v1/clusters/' + cluster, data=json.dumps(body))\n",
    "\n",
    "print(r.url)\n",
    "print(r.status_code)\n",
    "assert r.status_code == 200\n",
    "print(\"Configuration changed successfully!\")\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration hadoop-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"href\": \"http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06/configurations?type=hadoop-env&tag=version1436555345860\",\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"properties\": {\n",
      "        \"hdfs_user_keytab\": \"/etc/security/keytabs/hdfs.headless.keytab\",\n",
      "        \"namenode_heapsize\": \"4608m\",\n",
      "        \"hdfs_user\": \"hdfs\",\n",
      "        \"dtnode_heapsize\": \"1024m\",\n",
      "        \"namenode_opt_maxpermsize\": \"256m\",\n",
      "        \"content\": \"\\n# Set Hadoop-specific environment variables here.\\n\\n# The only required environment variable is JAVA_HOME.  All others are\\n# optional.  When running a distributed configuration it is best to\\n# set JAVA_HOME in this file, so that it is correctly defined on\\n# remote nodes.\\n\\n# The java implementation to use.  Required.\\nexport JAVA_HOME={{java_home}}\\nexport HADOOP_HOME_WARN_SUPPRESS=1\\n\\n# Hadoop home directory\\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\\n\\n# Hadoop Configuration Directory\\n\\n{# this is different for HDP1 #}\\n# Path to jsvc required by secure HDP 2.0 datanode\\nexport JSVC_HOME={{jsvc_path}}\\n\\n\\n# The maximum amount of heap to use, in MB. Default is 1000.\\nexport HADOOP_HEAPSIZE=\\\"{{hadoop_heapsize}}\\\"\\n\\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\\\"-Xms{{namenode_heapsize}}\\\"\\n\\n# Extra Java runtime options.  Empty by default.\\nexport HADOOP_OPTS=\\\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\\\"\\n\\n# Command specific options appended to HADOOP_OPTS when specified\\nHADOOP_JOBTRACKER_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\\\"\\n\\nHADOOP_TASKTRACKER_OPTS=\\\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\\\"\\n\\n{% if java_version < 8 %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\\\"\\n\\n{% else %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\\\"\\n{% endif %}\\n\\nHADOOP_NFS3_OPTS=\\\"-Xmx{{nfsgateway_heapsize}}m -Dhadoop.security.logger=ERROR,DRFAS ${HADOOP_NFS3_OPTS}\\\"\\nHADOOP_BALANCER_OPTS=\\\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\\\"\\n\\n\\n# On secure datanodes, user to run the datanode as after dropping privileges\\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\\n\\n# Extra ssh options.  Empty by default.\\nexport HADOOP_SSH_OPTS=\\\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\\\"\\n\\n# Where log files are stored.  $HADOOP_HOME/logs by default.\\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\\n\\n# History server logs\\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\\n\\n# Where log files are stored in the secure data environment.\\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\\n\\n# host:path where hadoop code should be rsync'd from.  Unset by default.\\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\\n\\n# Seconds to sleep between slave commands.  Unset by default.  This\\n# can be useful in large clusters, where, e.g., slave rsyncs can\\n# otherwise arrive faster than the master can service them.\\n# export HADOOP_SLAVE_SLEEP=0.1\\n\\n# The directory where pid files are stored. /tmp by default.\\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# History server pid\\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\\n\\nYARN_RESOURCEMANAGER_OPTS=\\\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\\\"\\n\\n# A string representing this instance of hadoop. $USER by default.\\nexport HADOOP_IDENT_STRING=$USER\\n\\n# The scheduling priority for daemon processes.  See 'man nice'.\\n\\n# export HADOOP_NICENESS=10\\n\\n# Use libraries from standard classpath\\nJAVA_JDBC_LIBS=\\\"\\\"\\n\\n#Add libraries required by mysql connector\\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Add libraries required by oracle connector\\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Setting path to hdfs command line\\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\\n\\n# Mostly required for hadoop 2.0\\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}\\n\\nexport HADOOP_OPTS=\\\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\\\"\\n    \",\n",
      "        \"hdfs_log_dir_prefix\": \"/var/log/hadoop\",\n",
      "        \"dfs.datanode.data.dir.mount.file\": \"/etc/hadoop/conf/dfs_data_dir_mount.hist\",\n",
      "        \"keyserver_host\": \" \",\n",
      "        \"proxyuser_group\": \"users\",\n",
      "        \"nfsgateway_heapsize\": \"1024\",\n",
      "        \"hadoop_heapsize\": \"1024\",\n",
      "        \"hadoop_root_logger\": \"INFO,RFA\",\n",
      "        \"namenode_opt_maxnewsize\": \"576m\",\n",
      "        \"hdfs_principal_name\": \"hdfs@HORTONWORKS.COM\",\n",
      "        \"keyserver_port\": \" \",\n",
      "        \"namenode_opt_permsize\": \"128m\",\n",
      "        \"namenode_opt_newsize\": \"576m\",\n",
      "        \"hadoop_pid_dir_prefix\": \"/var/run/hadoop\"\n",
      "      },\n",
      "      \"type\": \"hadoop-env\",\n",
      "      \"href\": \"http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06/configurations?type=hadoop-env&tag=version1436555345860\",\n",
      "      \"Config\": {\n",
      "        \"cluster_name\": \"p-lab06\",\n",
      "        \"stack_id\": \"HDP-2.3\"\n",
      "      },\n",
      "      \"tag\": \"version1436555345860\",\n",
      "      \"version\": 3\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Get current configuration tag\n",
    "config = 'hadoop-env'\n",
    "r = s.get(api_url + '/api/v1/clusters/' + cluster + '?fields=Clusters/desired_configs/' + config)\n",
    "assert r.status_code == 200\n",
    "tag = r.json()['Clusters']['desired_configs'][config]['tag']\n",
    "\n",
    "## Get current configuration\n",
    "r = s.get(api_url + '/api/v1/clusters/' + cluster + '/configurations?type=' + config + '&tag=' + tag)\n",
    "assert r.status_code == 200\n",
    "print(json.dumps(r.json(), indent=2))\n",
    "\n",
    "## Update config\n",
    "config_old = r.json()['items'][0]\n",
    "config_new = r.json()['items'][0]\n",
    "\n",
    "#### Make your changes here\n",
    "s = config_new['properties']['content']\n",
    "s = s + '\\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${JAVA_JDBC_LIBS}:\\n\\n'\n",
    "config_new['properties']['content'] = s\n",
    "\n",
    "#print(json.dumps(config_new, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "+++ \n",
      "@@ -5,7 +5,7 @@\n",
      "     \"hdfs_user\": \"hdfs\",\n",
      "     \"dtnode_heapsize\": \"1024m\",\n",
      "     \"namenode_opt_maxpermsize\": \"256m\",\n",
      "-    \"content\": \"\\n# Set Hadoop-specific environment variables here.\\n\\n# The only required environment variable is JAVA_HOME.  All others are\\n# optional.  When running a distributed configuration it is best to\\n# set JAVA_HOME in this file, so that it is correctly defined on\\n# remote nodes.\\n\\n# The java implementation to use.  Required.\\nexport JAVA_HOME={{java_home}}\\nexport HADOOP_HOME_WARN_SUPPRESS=1\\n\\n# Hadoop home directory\\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\\n\\n# Hadoop Configuration Directory\\n\\n{# this is different for HDP1 #}\\n# Path to jsvc required by secure HDP 2.0 datanode\\nexport JSVC_HOME={{jsvc_path}}\\n\\n\\n# The maximum amount of heap to use, in MB. Default is 1000.\\nexport HADOOP_HEAPSIZE=\\\"{{hadoop_heapsize}}\\\"\\n\\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\\\"-Xms{{namenode_heapsize}}\\\"\\n\\n# Extra Java runtime options.  Empty by default.\\nexport HADOOP_OPTS=\\\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\\\"\\n\\n# Command specific options appended to HADOOP_OPTS when specified\\nHADOOP_JOBTRACKER_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\\\"\\n\\nHADOOP_TASKTRACKER_OPTS=\\\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\\\"\\n\\n{% if java_version < 8 %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\\\"\\n\\n{% else %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\\\"\\n{% endif %}\\n\\nHADOOP_NFS3_OPTS=\\\"-Xmx{{nfsgateway_heapsize}}m -Dhadoop.security.logger=ERROR,DRFAS ${HADOOP_NFS3_OPTS}\\\"\\nHADOOP_BALANCER_OPTS=\\\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\\\"\\n\\n\\n# On secure datanodes, user to run the datanode as after dropping privileges\\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\\n\\n# Extra ssh options.  Empty by default.\\nexport HADOOP_SSH_OPTS=\\\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\\\"\\n\\n# Where log files are stored.  $HADOOP_HOME/logs by default.\\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\\n\\n# History server logs\\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\\n\\n# Where log files are stored in the secure data environment.\\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\\n\\n# host:path where hadoop code should be rsync'd from.  Unset by default.\\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\\n\\n# Seconds to sleep between slave commands.  Unset by default.  This\\n# can be useful in large clusters, where, e.g., slave rsyncs can\\n# otherwise arrive faster than the master can service them.\\n# export HADOOP_SLAVE_SLEEP=0.1\\n\\n# The directory where pid files are stored. /tmp by default.\\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# History server pid\\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\\n\\nYARN_RESOURCEMANAGER_OPTS=\\\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\\\"\\n\\n# A string representing this instance of hadoop. $USER by default.\\nexport HADOOP_IDENT_STRING=$USER\\n\\n# The scheduling priority for daemon processes.  See 'man nice'.\\n\\n# export HADOOP_NICENESS=10\\n\\n# Use libraries from standard classpath\\nJAVA_JDBC_LIBS=\\\"\\\"\\n\\n#Add libraries required by mysql connector\\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Add libraries required by oracle connector\\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Setting path to hdfs command line\\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\\n\\n# Mostly required for hadoop 2.0\\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}\\n\\nexport HADOOP_OPTS=\\\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\\\"\\n    \",\n",
      "+    \"content\": \"\\n# Set Hadoop-specific environment variables here.\\n\\n# The only required environment variable is JAVA_HOME.  All others are\\n# optional.  When running a distributed configuration it is best to\\n# set JAVA_HOME in this file, so that it is correctly defined on\\n# remote nodes.\\n\\n# The java implementation to use.  Required.\\nexport JAVA_HOME={{java_home}}\\nexport HADOOP_HOME_WARN_SUPPRESS=1\\n\\n# Hadoop home directory\\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\\n\\n# Hadoop Configuration Directory\\n\\n{# this is different for HDP1 #}\\n# Path to jsvc required by secure HDP 2.0 datanode\\nexport JSVC_HOME={{jsvc_path}}\\n\\n\\n# The maximum amount of heap to use, in MB. Default is 1000.\\nexport HADOOP_HEAPSIZE=\\\"{{hadoop_heapsize}}\\\"\\n\\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\\\"-Xms{{namenode_heapsize}}\\\"\\n\\n# Extra Java runtime options.  Empty by default.\\nexport HADOOP_OPTS=\\\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\\\"\\n\\n# Command specific options appended to HADOOP_OPTS when specified\\nHADOOP_JOBTRACKER_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\\\"\\n\\nHADOOP_TASKTRACKER_OPTS=\\\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\\\"\\n\\n{% if java_version < 8 %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\\\"\\n\\n{% else %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\\\"\\n{% endif %}\\n\\nHADOOP_NFS3_OPTS=\\\"-Xmx{{nfsgateway_heapsize}}m -Dhadoop.security.logger=ERROR,DRFAS ${HADOOP_NFS3_OPTS}\\\"\\nHADOOP_BALANCER_OPTS=\\\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\\\"\\n\\n\\n# On secure datanodes, user to run the datanode as after dropping privileges\\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\\n\\n# Extra ssh options.  Empty by default.\\nexport HADOOP_SSH_OPTS=\\\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\\\"\\n\\n# Where log files are stored.  $HADOOP_HOME/logs by default.\\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\\n\\n# History server logs\\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\\n\\n# Where log files are stored in the secure data environment.\\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\\n\\n# host:path where hadoop code should be rsync'd from.  Unset by default.\\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\\n\\n# Seconds to sleep between slave commands.  Unset by default.  This\\n# can be useful in large clusters, where, e.g., slave rsyncs can\\n# otherwise arrive faster than the master can service them.\\n# export HADOOP_SLAVE_SLEEP=0.1\\n\\n# The directory where pid files are stored. /tmp by default.\\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# History server pid\\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\\n\\nYARN_RESOURCEMANAGER_OPTS=\\\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\\\"\\n\\n# A string representing this instance of hadoop. $USER by default.\\nexport HADOOP_IDENT_STRING=$USER\\n\\n# The scheduling priority for daemon processes.  See 'man nice'.\\n\\n# export HADOOP_NICENESS=10\\n\\n# Use libraries from standard classpath\\nJAVA_JDBC_LIBS=\\\"\\\"\\n\\n#Add libraries required by mysql connector\\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Add libraries required by oracle connector\\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Setting path to hdfs command line\\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\\n\\n# Mostly required for hadoop 2.0\\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}\\n\\nexport HADOOP_OPTS=\\\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\\\"\\n    \\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${JAVA_JDBC_LIBS}:\\n\\n\",\n",
      "     \"hdfs_log_dir_prefix\": \"/var/log/hadoop\",\n",
      "     \"dfs.datanode.data.dir.mount.file\": \"/etc/hadoop/conf/dfs_data_dir_mount.hist\",\n",
      "     \"keyserver_host\": \" \",\n"
     ]
    }
   ],
   "source": [
    "#### Show the differences\n",
    "\n",
    "a = json.dumps(config_old, indent=2).splitlines(1)\n",
    "b = json.dumps(config_new, indent=2).splitlines(1)\n",
    "\n",
    "for line in difflib.unified_diff(a, b):\n",
    "     sys.stdout.write(line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Clusters\": {\n",
      "    \"desired_config\": {\n",
      "      \"properties\": {\n",
      "        \"hdfs_user_keytab\": \"/etc/security/keytabs/hdfs.headless.keytab\",\n",
      "        \"namenode_heapsize\": \"4608m\",\n",
      "        \"hdfs_user\": \"hdfs\",\n",
      "        \"dtnode_heapsize\": \"1024m\",\n",
      "        \"namenode_opt_maxpermsize\": \"256m\",\n",
      "        \"content\": \"\\n# Set Hadoop-specific environment variables here.\\n\\n# The only required environment variable is JAVA_HOME.  All others are\\n# optional.  When running a distributed configuration it is best to\\n# set JAVA_HOME in this file, so that it is correctly defined on\\n# remote nodes.\\n\\n# The java implementation to use.  Required.\\nexport JAVA_HOME={{java_home}}\\nexport HADOOP_HOME_WARN_SUPPRESS=1\\n\\n# Hadoop home directory\\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\\n\\n# Hadoop Configuration Directory\\n\\n{# this is different for HDP1 #}\\n# Path to jsvc required by secure HDP 2.0 datanode\\nexport JSVC_HOME={{jsvc_path}}\\n\\n\\n# The maximum amount of heap to use, in MB. Default is 1000.\\nexport HADOOP_HEAPSIZE=\\\"{{hadoop_heapsize}}\\\"\\n\\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\\\"-Xms{{namenode_heapsize}}\\\"\\n\\n# Extra Java runtime options.  Empty by default.\\nexport HADOOP_OPTS=\\\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\\\"\\n\\n# Command specific options appended to HADOOP_OPTS when specified\\nHADOOP_JOBTRACKER_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\\\"\\n\\nHADOOP_TASKTRACKER_OPTS=\\\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\\\"\\n\\n{% if java_version < 8 %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\\\"\\n\\n{% else %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\\\"\\n{% endif %}\\n\\nHADOOP_NFS3_OPTS=\\\"-Xmx{{nfsgateway_heapsize}}m -Dhadoop.security.logger=ERROR,DRFAS ${HADOOP_NFS3_OPTS}\\\"\\nHADOOP_BALANCER_OPTS=\\\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\\\"\\n\\n\\n# On secure datanodes, user to run the datanode as after dropping privileges\\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\\n\\n# Extra ssh options.  Empty by default.\\nexport HADOOP_SSH_OPTS=\\\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\\\"\\n\\n# Where log files are stored.  $HADOOP_HOME/logs by default.\\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\\n\\n# History server logs\\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\\n\\n# Where log files are stored in the secure data environment.\\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\\n\\n# host:path where hadoop code should be rsync'd from.  Unset by default.\\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\\n\\n# Seconds to sleep between slave commands.  Unset by default.  This\\n# can be useful in large clusters, where, e.g., slave rsyncs can\\n# otherwise arrive faster than the master can service them.\\n# export HADOOP_SLAVE_SLEEP=0.1\\n\\n# The directory where pid files are stored. /tmp by default.\\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# History server pid\\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\\n\\nYARN_RESOURCEMANAGER_OPTS=\\\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\\\"\\n\\n# A string representing this instance of hadoop. $USER by default.\\nexport HADOOP_IDENT_STRING=$USER\\n\\n# The scheduling priority for daemon processes.  See 'man nice'.\\n\\n# export HADOOP_NICENESS=10\\n\\n# Use libraries from standard classpath\\nJAVA_JDBC_LIBS=\\\"\\\"\\n\\n#Add libraries required by mysql connector\\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Add libraries required by oracle connector\\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Setting path to hdfs command line\\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\\n\\n# Mostly required for hadoop 2.0\\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}\\n\\nexport HADOOP_OPTS=\\\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\\\"\\n    \\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${JAVA_JDBC_LIBS}:\\n\\n\",\n",
      "        \"hdfs_log_dir_prefix\": \"/var/log/hadoop\",\n",
      "        \"dfs.datanode.data.dir.mount.file\": \"/etc/hadoop/conf/dfs_data_dir_mount.hist\",\n",
      "        \"keyserver_host\": \" \",\n",
      "        \"proxyuser_group\": \"users\",\n",
      "        \"nfsgateway_heapsize\": \"1024\",\n",
      "        \"hadoop_heapsize\": \"1024\",\n",
      "        \"hadoop_root_logger\": \"INFO,RFA\",\n",
      "        \"namenode_opt_maxnewsize\": \"576m\",\n",
      "        \"hdfs_principal_name\": \"hdfs@HORTONWORKS.COM\",\n",
      "        \"keyserver_port\": \" \",\n",
      "        \"namenode_opt_permsize\": \"128m\",\n",
      "        \"namenode_opt_newsize\": \"576m\",\n",
      "        \"hadoop_pid_dir_prefix\": \"/var/run/hadoop\"\n",
      "      },\n",
      "      \"type\": \"hadoop-env\",\n",
      "      \"tag\": \"version1436641670436791808\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#### Manipulate the document to match the format Ambari expects\n",
    "\n",
    "#### Adds new configuration tag, deletes fields, and wraps in appropriate json\n",
    "config_new['tag'] = 'version' + str(int(round(time.time() * 1000000000)))\n",
    "del config_new['Config']\n",
    "del config_new['href']\n",
    "del config_new['version']\n",
    "config_new = {\"Clusters\": {\"desired_config\": config_new}}\n",
    "\n",
    "print(json.dumps(config_new, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06\n",
      "200\n",
      "Configuration changed successfully!\n",
      "{\n",
      "  \"resources\": [\n",
      "    {\n",
      "      \"configurations\": [\n",
      "        {\n",
      "          \"stackId\": {\n",
      "            \"stackId\": \"HDP-2.3\",\n",
      "            \"stackName\": \"HDP\",\n",
      "            \"stackVersion\": \"2.3\"\n",
      "          },\n",
      "          \"type\": \"hadoop-env\",\n",
      "          \"versionTag\": \"version1436641670436791808\",\n",
      "          \"configs\": {\n",
      "            \"hdfs_user_keytab\": \"/etc/security/keytabs/hdfs.headless.keytab\",\n",
      "            \"namenode_heapsize\": \"4608m\",\n",
      "            \"hdfs_user\": \"hdfs\",\n",
      "            \"namenode_opt_permsize\": \"128m\",\n",
      "            \"keyserver_port\": \" \",\n",
      "            \"content\": \"\\n# Set Hadoop-specific environment variables here.\\n\\n# The only required environment variable is JAVA_HOME.  All others are\\n# optional.  When running a distributed configuration it is best to\\n# set JAVA_HOME in this file, so that it is correctly defined on\\n# remote nodes.\\n\\n# The java implementation to use.  Required.\\nexport JAVA_HOME={{java_home}}\\nexport HADOOP_HOME_WARN_SUPPRESS=1\\n\\n# Hadoop home directory\\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\\n\\n# Hadoop Configuration Directory\\n\\n{# this is different for HDP1 #}\\n# Path to jsvc required by secure HDP 2.0 datanode\\nexport JSVC_HOME={{jsvc_path}}\\n\\n\\n# The maximum amount of heap to use, in MB. Default is 1000.\\nexport HADOOP_HEAPSIZE=\\\"{{hadoop_heapsize}}\\\"\\n\\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\\\"-Xms{{namenode_heapsize}}\\\"\\n\\n# Extra Java runtime options.  Empty by default.\\nexport HADOOP_OPTS=\\\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\\\"\\n\\n# Command specific options appended to HADOOP_OPTS when specified\\nHADOOP_JOBTRACKER_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\\\"\\n\\nHADOOP_TASKTRACKER_OPTS=\\\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\\\"\\n\\n{% if java_version < 8 %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\\\"\\n\\n{% else %}\\nSHARED_HADOOP_NAMENODE_OPTS=\\\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT\\\"\\nexport HADOOP_NAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-namenode/bin/kill-name-node\\\\\\\" -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\\\"\\nexport HADOOP_DATANODE_OPTS=\\\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\\\"\\n\\nexport HADOOP_SECONDARYNAMENODE_OPTS=\\\"${SHARED_HADOOP_NAMENODE_OPTS} -XX:OnOutOfMemoryError=\\\\\\\"/usr/hdp/current/hadoop-hdfs-secondarynamenode/bin/kill-secondary-name-node\\\\\\\" ${HADOOP_SECONDARYNAMENODE_OPTS}\\\"\\n\\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\\nexport HADOOP_CLIENT_OPTS=\\\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\\\"\\n{% endif %}\\n\\nHADOOP_NFS3_OPTS=\\\"-Xmx{{nfsgateway_heapsize}}m -Dhadoop.security.logger=ERROR,DRFAS ${HADOOP_NFS3_OPTS}\\\"\\nHADOOP_BALANCER_OPTS=\\\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\\\"\\n\\n\\n# On secure datanodes, user to run the datanode as after dropping privileges\\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\\n\\n# Extra ssh options.  Empty by default.\\nexport HADOOP_SSH_OPTS=\\\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\\\"\\n\\n# Where log files are stored.  $HADOOP_HOME/logs by default.\\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\\n\\n# History server logs\\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\\n\\n# Where log files are stored in the secure data environment.\\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\\n\\n# host:path where hadoop code should be rsync'd from.  Unset by default.\\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\\n\\n# Seconds to sleep between slave commands.  Unset by default.  This\\n# can be useful in large clusters, where, e.g., slave rsyncs can\\n# otherwise arrive faster than the master can service them.\\n# export HADOOP_SLAVE_SLEEP=0.1\\n\\n# The directory where pid files are stored. /tmp by default.\\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\\n\\n# History server pid\\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\\n\\nYARN_RESOURCEMANAGER_OPTS=\\\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\\\"\\n\\n# A string representing this instance of hadoop. $USER by default.\\nexport HADOOP_IDENT_STRING=$USER\\n\\n# The scheduling priority for daemon processes.  See 'man nice'.\\n\\n# export HADOOP_NICENESS=10\\n\\n# Use libraries from standard classpath\\nJAVA_JDBC_LIBS=\\\"\\\"\\n\\n#Add libraries required by mysql connector\\nfor jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Add libraries required by oracle connector\\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\\ndo\\n  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\\ndone\\n\\n# Setting path to hdfs command line\\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\\n\\n# Mostly required for hadoop 2.0\\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}\\n\\nexport HADOOP_OPTS=\\\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\\\"\\n    \\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${JAVA_JDBC_LIBS}:\\n\\n\",\n",
      "            \"hdfs_log_dir_prefix\": \"/var/log/hadoop\",\n",
      "            \"dfs.datanode.data.dir.mount.file\": \"/etc/hadoop/conf/dfs_data_dir_mount.hist\",\n",
      "            \"keyserver_host\": \" \",\n",
      "            \"hadoop_pid_dir_prefix\": \"/var/run/hadoop\",\n",
      "            \"nfsgateway_heapsize\": \"1024\",\n",
      "            \"hadoop_heapsize\": \"1024\",\n",
      "            \"hadoop_root_logger\": \"INFO,RFA\",\n",
      "            \"namenode_opt_maxnewsize\": \"576m\",\n",
      "            \"hdfs_principal_name\": \"hdfs@HORTONWORKS.COM\",\n",
      "            \"namenode_opt_maxpermsize\": \"256m\",\n",
      "            \"dtnode_heapsize\": \"1024m\",\n",
      "            \"proxyuser_group\": \"users\",\n",
      "            \"namenode_opt_newsize\": \"576m\"\n",
      "          },\n",
      "          \"serviceConfigVersions\": null,\n",
      "          \"version\": 4,\n",
      "          \"clusterName\": \"p-lab06\",\n",
      "          \"configAttributes\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"service_config_version_note\": null,\n",
      "      \"group_id\": -1,\n",
      "      \"href\": \"http://p-lab06.cloudapp.net:8080/api/v1/clusters/p-lab06/configurations/service_config_versions?service_name=HDFS&service_config_version=9\",\n",
      "      \"service_name\": \"HDFS\",\n",
      "      \"service_config_version\": 9,\n",
      "      \"group_name\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "body = config_new\n",
    "r = s.put(api_url + '/api/v1/clusters/' + cluster, data=json.dumps(body))\n",
    "\n",
    "print(r.url)\n",
    "print(r.status_code)\n",
    "assert r.status_code == 200\n",
    "print(\"Configuration changed successfully!\")\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
